{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "For this, you are going to copy the Vaihingen dataset in your own Google drive. Access the folder [here](https://drive.google.com/drive/folders/1Tr3q8kjPDzoamNFuHv7vTBsj5-acJC7Z?usp=sharing) and copy it to your drive.\n"
      ],
      "metadata": {
        "id": "ZdWQx3EGT0L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw9R3OmLlU9X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -F \"/content/drive/MyDrive/Workshop_Semantic_Segmentation/DataVaihingen/top/\""
      ],
      "metadata": {
        "id": "ziJS9fZDlYDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = '/content/drive/MyDrive/Workshop_Semantic_Segmentation'"
      ],
      "metadata": {
        "id": "iYoSKS9plawf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define a consistent root directory for all data files (Fixes NameError: DATA_ROOT_DIR)\n",
        "DATA_ROOT_DIR = os.path.join(base_folder, 'DataVaihingen')\n",
        "\n",
        "print(f\"DATA_ROOT_DIR set to: {DATA_ROOT_DIR}\")"
      ],
      "metadata": {
        "id": "yAipff0klasr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# --- Define standard Vaihingen image splits ---\n",
        "\n",
        "# List of image area numbers to be used for training\n",
        "TRAIN_AREAS = [1, 3, 5, 7, 11, 13, 15, 21, 23, 26, 30, 32, 34, 37]\n",
        "# List of image area numbers to be used for validation (often called 'Test' in the data)\n",
        "VAL_AREAS = [2, 4, 6, 8, 10, 12, 14, 16, 20, 22, 24, 27, 29, 31, 33, 35, 38]\n",
        "\n",
        "def generate_file_list(data_root_dir, areas, output_filename):\n",
        "    \"\"\"Generates a text file listing all image names for the given areas.\"\"\"\n",
        "\n",
        "    # 1. Look for all .tif files in the 'top' directory\n",
        "    img_dir = os.path.join(data_root_dir, 'top')\n",
        "    if not os.path.exists(img_dir):\n",
        "        print(f\"Error: Image directory not found at {img_dir}. Check your DataVaihingen structure.\")\n",
        "        return\n",
        "\n",
        "    all_files = os.listdir(img_dir)\n",
        "    image_list = []\n",
        "\n",
        "    # 2. Filter files to include only those matching the target areas\n",
        "    for area in areas:\n",
        "        # Standard file naming convention: top_mosaic_09cm_area[number].tif\n",
        "        filename = f'top_mosaic_09cm_area{area}.tif'\n",
        "        if filename in all_files:\n",
        "            image_list.append(filename)\n",
        "\n",
        "    # 3. Write the list to the specified output file\n",
        "    output_path = os.path.join(data_root_dir, output_filename)\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write('\\n'.join(image_list) + '\\n')\n",
        "\n",
        "    print(f\"Generated {output_filename} with {len(image_list)} files at: {output_path}\")\n",
        "\n",
        "\n",
        "# --- Run Generation ---\n",
        "print(f\"Using DATA_ROOT_DIR: {DATA_ROOT_DIR}\")\n",
        "\n",
        "# Generate training list\n",
        "generate_file_list(DATA_ROOT_DIR, TRAIN_AREAS, 'train_set.txt')\n",
        "\n",
        "# Generate validation list\n",
        "generate_file_list(DATA_ROOT_DIR, VAL_AREAS, 'val_set.txt')"
      ],
      "metadata": {
        "id": "yH1x4TYdlaqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Hypercolumns(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(Hypercolumns, self).__init__()\n",
        "\n",
        "        # Encoder (VGG-like structure)\n",
        "        # Block 1: Output size 256x256\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Block 2: Output size 128x128\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Block 3: Output size 64x64\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Block 4: Output size 32x32\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Block 5: Output size 16x16\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # 1x1 convolution layers for hypercolumns\n",
        "        self.fc1 = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(128, num_classes, kernel_size=1)\n",
        "        self.fc3 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "        self.fc4 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "        self.fc5 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Block 1\n",
        "        x = self.relu1_1(self.conv1_1(x))\n",
        "        x = self.relu1_2(self.conv1_2(x))\n",
        "        c1 = x # Feature map 1\n",
        "        x, id1 = self.pool1(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = self.relu2_1(self.conv2_1(x))\n",
        "        x = self.relu2_2(self.conv2_2(x))\n",
        "        c2 = x # Feature map 2\n",
        "        x, id2 = self.pool2(x)\n",
        "\n",
        "        # Block 3\n",
        "        x = self.relu3_1(self.conv3_1(x))\n",
        "        x = self.relu3_2(self.conv3_2(x))\n",
        "        x = self.relu3_3(self.conv3_3(x))\n",
        "        c3 = x # Feature map 3\n",
        "        x, id3 = self.pool3(x)\n",
        "\n",
        "        # Block 4\n",
        "        x = self.relu4_1(self.conv4_1(x))\n",
        "        x = self.relu4_2(self.conv4_2(x))\n",
        "        x = self.relu4_3(self.conv4_3(x))\n",
        "        c4 = x # Feature map 4\n",
        "        x, id4 = self.pool4(x)\n",
        "\n",
        "        # Block 5\n",
        "        x = self.relu5_1(self.conv5_1(x))\n",
        "        x = self.relu5_2(self.conv5_2(x))\n",
        "        x = self.relu5_3(self.conv5_3(x))\n",
        "        c5 = x # Feature map 5\n",
        "\n",
        "        # Hypercolumn layers\n",
        "\n",
        "        # c1 is 64x channels, 1/1 size\n",
        "        hc1 = self.fc1(c1)\n",
        "\n",
        "        # c2 is 128x channels, 1/2 size -> upscale to 1/1\n",
        "        hc2 = F.interpolate(self.fc2(c2), size=c1.size()[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # c3 is 256x channels, 1/4 size -> upscale to 1/1\n",
        "        hc3 = F.interpolate(self.fc3(c3), size=c1.size()[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # c4 is 512x channels, 1/8 size -> upscale to 1/1\n",
        "        hc4 = F.interpolate(self.fc4(c4), size=c1.size()[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # c5 is 512x channels, 1/16 size -> upscale to 1/1\n",
        "        hc5 = F.interpolate(self.fc5(c5), size=c1.size()[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Combine hypercolumns\n",
        "        hypercolumn_output = hc1 + hc2 + hc3 + hc4 + hc5\n",
        "\n",
        "        return hypercolumn_output"
      ],
      "metadata": {
        "id": "TMH_Ll1tlan-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "class VaihingenDataset(Dataset):\n",
        "    def __init__(self, img_folder, GT_folder, split='train', patch_size=256):\n",
        "\n",
        "        # Define the full set of tiles\n",
        "        all_train_tiles = [1, 3, 5, 7, 11, 13, 15, 17, 21, 23, 26, 28, 30, 32, 34, 37]\n",
        "        all_val_tiles = [2, 4, 6, 8, 10, 12, 14, 16, 20, 22, 27, 29, 31, 33, 35, 38]\n",
        "\n",
        "        if split == 'train':\n",
        "            tiles_to_check = all_train_tiles\n",
        "        elif split == 'val':\n",
        "            tiles_to_check = all_val_tiles\n",
        "        elif split == 'test':\n",
        "            tiles_to_check = [2, 4, 6]\n",
        "        else:\n",
        "            raise ValueError(\"Split must be 'train', 'val', or 'test'\")\n",
        "\n",
        "        self.img_folder = img_folder\n",
        "        self.GT_folder = GT_folder\n",
        "        self.patch_size = patch_size\n",
        "        self.split = split\n",
        "\n",
        "        potential_files = []\n",
        "        for i in tiles_to_check:\n",
        "            fn = f'top_mosaic_09cm_area{i}.tif'\n",
        "            if os.path.exists(os.path.join(img_folder, fn)) and os.path.exists(os.path.join(GT_folder, fn)):\n",
        "                potential_files.append(fn)\n",
        "            else:\n",
        "                print(f\"Warning: Tile {i} ({fn}) is missing and will be skipped.\")\n",
        "\n",
        "        if not potential_files:\n",
        "            raise FileNotFoundError(f\"No files found for split '{split}'. Check your data files.\")\n",
        "\n",
        "        loaded_images = [io.imread(os.path.join(img_folder, fn)) for fn in potential_files]\n",
        "        loaded_GTs = [io.imread(os.path.join(GT_folder, fn)) for fn in potential_files]\n",
        "\n",
        "        # --- CRITICAL FIX: Robust Shape and Patch Generation ---\n",
        "        self.images = []\n",
        "        self.GTs = []\n",
        "        self.patch_locations = []\n",
        "        new_tile_idx = 0\n",
        "\n",
        "        for idx, img in enumerate(loaded_images):\n",
        "            GT = loaded_GTs[idx]\n",
        "            tile_name = potential_files[idx]\n",
        "\n",
        "            # 1. Robust Image Shape Check\n",
        "            try:\n",
        "                H_img, W_img, C = img.shape\n",
        "            except ValueError:\n",
        "                print(f\"Warning: Skipping tile {tile_name} (Image) due to malformed shape (H, W, C not found).\")\n",
        "                continue\n",
        "\n",
        "            # 2. Robust GT Shape Check\n",
        "            try:\n",
        "                if len(GT.shape) == 2:\n",
        "                    H_gt, W_gt = GT.shape\n",
        "                else:\n",
        "                    # Attempt to handle a 3D GT file (e.g., (H, W, 1))\n",
        "                    H_gt, W_gt, _ = GT.shape\n",
        "            except ValueError:\n",
        "                print(f\"Warning: Skipping tile {tile_name} (GT) due to malformed shape.\")\n",
        "                continue\n",
        "\n",
        "            # 3. Compatibility Check (The source of the [0, 256] error)\n",
        "            if H_img != H_gt or W_img != W_gt:\n",
        "                print(f\"Warning: Skipping tile {tile_name}. Image ({H_img}x{W_img}) and GT ({H_gt}x{W_gt}) shapes are incompatible.\")\n",
        "                continue\n",
        "\n",
        "            H, W = H_img, W_img\n",
        "\n",
        "            # 4. Size Check\n",
        "            if H < self.patch_size or W < self.patch_size:\n",
        "                print(f\"Warning: Skipping tile {tile_name} due to size {H}x{W}. Too small for {self.patch_size} patch.\")\n",
        "                continue\n",
        "\n",
        "            # If all checks pass, proceed to store and generate patches\n",
        "            self.images.append(img)\n",
        "            self.GTs.append(GT)\n",
        "\n",
        "            # Generate patch coordinates for this valid tile\n",
        "            rows = np.arange(0, H - self.patch_size + 1, self.patch_size)\n",
        "            cols = np.arange(0, W - self.patch_size + 1, self.patch_size)\n",
        "\n",
        "            for r in rows:\n",
        "                for c in cols:\n",
        "                    self.patch_locations.append((new_tile_idx, r, c))\n",
        "\n",
        "            new_tile_idx += 1\n",
        "\n",
        "\n",
        "        print(f\"Dataset '{split}' initialized with {len(self.images)} valid tiles and {len(self.patch_locations)} total patches.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patch_locations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tile_idx, r, c = self.patch_locations[idx]\n",
        "\n",
        "        img = self.images[tile_idx]\n",
        "        GT = self.GTs[tile_idx]\n",
        "\n",
        "        # Extract the patch (only the first 3 channels for RGB)\n",
        "        patch_img = img[r:r + self.patch_size, c:c + self.patch_size, :3]\n",
        "        patch_GT = GT[r:r + self.patch_size, c:c + self.patch_size]\n",
        "\n",
        "        # Preprocessing\n",
        "        patch_img = patch_img.astype(np.float32) / 255.0\n",
        "        patch_img = np.transpose(patch_img, (2, 0, 1))\n",
        "\n",
        "        img_tensor = torch.from_numpy(patch_img)\n",
        "        GT_tensor = torch.from_numpy(patch_GT).long()\n",
        "\n",
        "        return img_tensor, GT_tensor"
      ],
      "metadata": {
        "id": "TcljOOyqlalf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# =================================================================\n",
        "# 1. METRICS FUNCTIONS\n",
        "# =================================================================\n",
        "\n",
        "def compute_confusion_matrix(pred, target, num_classes):\n",
        "    \"\"\"Computes the confusion matrix.\"\"\"\n",
        "    pred = pred.reshape(-1)\n",
        "    target = target.reshape(-1)\n",
        "\n",
        "    # Filter out the invalid class\n",
        "    valid_mask = (target >= 0) & (target < num_classes)\n",
        "    pred = pred[valid_mask]\n",
        "    target = target[valid_mask]\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for t, p in zip(target.cpu().numpy(), pred.cpu().numpy()):\n",
        "        conf_matrix[t, p] += 1\n",
        "\n",
        "    return conf_matrix\n",
        "\n",
        "def compute_metrics(conf_matrix):\n",
        "    \"\"\"Computes standard segmentation metrics from the confusion matrix.\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    TP = np.diag(conf_matrix)\n",
        "    FP = conf_matrix.sum(axis=0) - TP\n",
        "    FN = conf_matrix.sum(axis=1) - TP\n",
        "    T = conf_matrix.sum()\n",
        "\n",
        "    # Overall Accuracy (OA)\n",
        "    metrics['Overall Accuracy'] = TP.sum() / T\n",
        "\n",
        "    # Recall (Per-Class Accuracy)\n",
        "    metrics['Recall'] = TP / (TP + FN + 1e-12)\n",
        "\n",
        "    # Precision\n",
        "    metrics['Precision'] = TP / (TP + FP + 1e-12)\n",
        "\n",
        "    # F1 Score\n",
        "    metrics['F1 Score'] = 2 * (metrics['Precision'] * metrics['Recall']) / (metrics['Precision'] + metrics['Recall'] + 1e-12)\n",
        "\n",
        "    # Mean F1\n",
        "    metrics['Mean F1'] = metrics['F1 Score'].mean()\n",
        "\n",
        "    # Intersection over Union (IoU)\n",
        "    metrics['IoU'] = TP / (TP + FP + FN + 1e-12)\n",
        "\n",
        "    # Mean IoU\n",
        "    metrics['Mean IoU'] = metrics['IoU'].mean()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def pretty_print_metrics(metrics):\n",
        "    \"\"\"Prints the metrics in a readable table format.\"\"\"\n",
        "    # NOTE: class_names are hardcoded for the Vaihingen dataset\n",
        "    class_names = [\n",
        "        \"Impervious Surf.\", \"Building\", \"Low Vegetation\",\n",
        "        \"Tree\", \"Car\", \"Clutter/Background\"\n",
        "    ]\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(f\"Overall Accuracy: {metrics['Overall Accuracy'] * 100:.2f}%\")\n",
        "    print(f\"Mean F1 Score:    {metrics['Mean F1'] * 100:.2f}%\")\n",
        "    print(f\"Mean IoU:         {metrics['Mean IoU'] * 100:.2f}%\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Per-Class Metrics:\")\n",
        "\n",
        "    header = f\"{'Class':<20} | {'IoU':<8} | {'F1 Score':<10}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        iou = metrics['IoU'][i] * 100\n",
        "        f1 = metrics['F1 Score'][i] * 100\n",
        "        print(f\"{name:<20} | {iou:<8.2f} | {f1:<10.2f}\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# 2. TRAINING AND VALIDATION FUNCTIONS\n",
        "# =================================================================\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Performs one epoch of training.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def val_model(model, val_loader, criterion, device, class_labels):\n",
        "    \"\"\"Performs validation and calculates loss and IoU.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    conf_matrix = np.zeros((len(class_labels), len(class_labels)), dtype=np.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Get predicted class (index of max probability)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Update confusion matrix (Fixes NameError if not imported/defined)\n",
        "            conf_matrix += compute_confusion_matrix(preds, labels, len(class_labels))\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = compute_metrics(conf_matrix)\n",
        "    mean_iou = metrics['Mean IoU']\n",
        "\n",
        "    return epoch_loss, mean_iou\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# 3. INFERENCE/PREDICTION FUNCTION\n",
        "# =================================================================\n",
        "\n",
        "def produce_results(image_name):\n",
        "    \"\"\"\n",
        "    Inference function to load an image, predict, and display results.\n",
        "    NOTE: Requires DATA_ROOT_DIR and base_folder to be correctly defined.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import skimage.io as io\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # --- Corrected Path Logic (Uses globally defined DATA_ROOT_DIR) ---\n",
        "    image_path = os.path.join(DATA_ROOT_DIR, 'top', image_name)\n",
        "    GT_path    = os.path.join(DATA_ROOT_DIR, '1CGT', image_name)\n",
        "    # --- End Corrected Path Logic ---\n",
        "\n",
        "    # Load data\n",
        "    image = io.imread(image_path)\n",
        "    image_rgb = image[:, :, :3]\n",
        "    GT = io.imread(GT_path).astype(int)\n",
        "\n",
        "    # Preprocessing for the model\n",
        "    img_tensor = image_rgb.astype(np.float32) / 255.0\n",
        "    img_tensor = np.transpose(img_tensor, (2, 0, 1))\n",
        "    img_tensor = torch.from_numpy(img_tensor).unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        output = network(img_tensor)\n",
        "\n",
        "    # Post-processing\n",
        "    output = output.squeeze().cpu()\n",
        "\n",
        "    _, predicted_mask = torch.max(output, 0)\n",
        "\n",
        "    predicted_mask = predicted_mask.numpy()\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.title(f'Original Image: {image_name}')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(GT)\n",
        "    plt.title('Ground Truth')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(predicted_mask)\n",
        "    plt.title('Predicted Mask')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BOEgUOQtlajT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "# NOTE: VaihingenDataset and Hypercolumns are assumed to be defined in previous cells (Code 7 and Code 6)\n",
        "\n",
        "# =========================================================\n",
        "# 1. Configuration and Paths\n",
        "# =========================================================\n",
        "\n",
        "# Paths confirmed in previous steps. Ensure these paths are correct in your Google Drive.\n",
        "DATA_ROOT_DIR = \"/content/drive/MyDrive/Workshop_Semantic_Segmentation/DataVaihingen\"\n",
        "base_folder = \"/content/drive/MyDrive/Workshop_Semantic_Segmentation\"\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define Hyperparameters\n",
        "class_labels = [0, 1, 2, 3, 4, 5]\n",
        "patch_size = 256\n",
        "batch_size = 4\n",
        "number_epochs = 25 # Changed to 25 based on your training loop code\n",
        "\n",
        "# Define the folder paths within the root directory\n",
        "IMG_FOLDER = os.path.join(DATA_ROOT_DIR, 'top')\n",
        "GT_FOLDER = os.path.join(DATA_ROOT_DIR, '1CGT')\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2. Initialize Data Loaders (CRITICAL FIX APPLIED)\n",
        "# =========================================================\n",
        "\n",
        "# Create Datasets (Uses the fixed VaihingenDataset from Code 7)\n",
        "train_data = VaihingenDataset(IMG_FOLDER, GT_FOLDER, split='train', patch_size=patch_size)\n",
        "val_data = VaihingenDataset(IMG_FOLDER, GT_FOLDER, split='val', patch_size=patch_size)\n",
        "\n",
        "# Create DataLoaders\n",
        "# *** CRITICAL FIX: Setting num_workers=0 to prevent the RuntimeError ***\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. Initialize Model, Loss, and Optimizer\n",
        "# =========================================================\n",
        "\n",
        "# Initialize Model (Hypercolumns is assumed to be defined in Code 6)\n",
        "network = Hypercolumns(num_classes=len(class_labels))\n",
        "network.to(device)\n",
        "print(\"Network initialized and moved to device.\")\n",
        "\n",
        "# Initialize Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Criterion and Optimizer initialized.\")"
      ],
      "metadata": {
        "id": "xGgwpQEGlafa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# NOTE: This code assumes that the network, loaders, criterion, optimizer,\n",
        "# and the correct training/validation functions (train_model, val_model)\n",
        "# have been initialized and defined in the preceding cells (Code 8 and Code 9).\n",
        "\n",
        "# =========================================================================\n",
        "# SETUP\n",
        "# =========================================================================\n",
        "\n",
        "# Variables (Assumed from Code 9)\n",
        "number_epochs = 25\n",
        "base_folder = \"/content/drive/MyDrive/Workshop_Semantic_Segmentation\"\n",
        "# device, network, train_loader, val_loader, criterion, optimizer, class_labels are ready\n",
        "\n",
        "# Setup for Model Saving\n",
        "save_directory = os.path.join(base_folder, 'WeightsVaihingen')\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "best_model_path = os.path.join(save_directory, 'best_hypercolumns_model.pth')\n",
        "\n",
        "# Metric Initialization\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_ious = []\n",
        "best_iou = -1.0\n",
        "\n",
        "# =========================================================================\n",
        "# BASELINE TRAINING LOOP (25 EPOCHS)\n",
        "# =========================================================================\n",
        "\n",
        "print(f\"Starting baseline training for {number_epochs} epochs...\")\n",
        "for epoch in range(number_epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{number_epochs} ---\")\n",
        "\n",
        "    # Training step (uses the correct train_model function from Code 8)\n",
        "    train_loss = train_model(network, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validation step (uses the correct val_model function from Code 8)\n",
        "    val_loss, val_iou = val_model(network, val_loader, criterion, device, class_labels)\n",
        "    val_losses.append(val_loss)\n",
        "    val_ious.append(val_iou)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Mean IoU: {val_iou:.4f}\")\n",
        "\n",
        "    # Save the best model based on Mean IoU\n",
        "    if val_iou > best_iou:\n",
        "        best_iou = val_iou\n",
        "        # Save model state dict to the designated Drive location\n",
        "        torch.save(network.state_dict(), best_model_path)\n",
        "        print(f\"Model saved to {best_model_path} with new best IoU: {best_iou:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")"
      ],
      "metadata": {
        "id": "R3hsWpATlacN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm # tqdm was imported in Cell 6\n",
        "\n",
        "# Assuming the variables (network, train_loader, val_loader, criterion, optimizer, device) are all defined\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_ious = []\n",
        "best_iou = -1.0\n",
        "best_model_path = 'best_hypercolumns_model.pth' # The path where your best model weights will be saved\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(number_epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{number_epochs} ---\")\n",
        "\n",
        "    # Training step (uses train_model from Cell 6)\n",
        "    train_loss = train_model(network, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validation step (uses val_model from Cell 6)\n",
        "    val_loss, val_iou = val_model(network, val_loader, criterion, device, class_labels)\n",
        "    val_losses.append(val_loss)\n",
        "    val_ious.append(val_iou)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Mean IoU: {val_iou:.4f}\")\n",
        "\n",
        "    # Save the best model based on Mean IoU\n",
        "    if val_iou > best_iou:\n",
        "        best_iou = val_iou\n",
        "        # Save model state dict\n",
        "        torch.save(network.state_dict(), best_model_path)\n",
        "        print(f\"Model saved to {best_model_path} with new best IoU: {best_iou:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")"
      ],
      "metadata": {
        "id": "kDWeC_A0laZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## corrected code\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# NOTE: This function relies on 'compute_confusion_matrix' and 'compute_metrics'\n",
        "# which must be defined in your metric calculation cells.\n",
        "\n",
        "def val_model(network, val_loader, criterion, device, class_labels):\n",
        "    # 1. Setup\n",
        "    network.eval() # Set the model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    num_classes = len(class_labels)\n",
        "\n",
        "    # Initialize a fresh confusion matrix for this epoch's validation run\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations for validation\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # 2. Forward Pass and Loss\n",
        "            # Using the criterion passed into the function (the unweighted one for baseline)\n",
        "            outputs = network(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            # 3. Get Predictions\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # 4. Accumulate Confusion Matrix\n",
        "            # This function must be defined in your metrics cells\n",
        "            confusion_matrix += compute_confusion_matrix(preds, labels, num_classes)\n",
        "\n",
        "    # 5. Calculate Final Metrics (Mean IoU)\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # This function must be defined in your metrics cells and return a dictionary\n",
        "    metrics = compute_metrics(confusion_matrix)\n",
        "    # The compute_metrics function likely returns IoU as a percentage (38.60),\n",
        "    # so we divide by 100.0 to return a float for correct comparison with best_iou = -1.0\n",
        "    mean_iou = metrics['Mean IoU'] / 100.0\n",
        "\n",
        "    # Return the two required values\n",
        "    return avg_val_loss, mean_iou"
      ],
      "metadata": {
        "id": "yZ7DM-gqlaV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Plot Loss ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss History Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Cross Entropy)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# --- Plot IoU ---\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_ious, label='Validation Mean IoU', color='red')\n",
        "plt.title('Validation Mean IoU Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean IoU')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1pQGICZylaTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Load the Best Model Weights\n",
        "# NOTE: Ensure best_model_path points to the correct location (e.g., on Drive if you used base_folder)\n",
        "print(f\"Loading weights from: {best_model_path}\")\n",
        "network.load_state_dict(torch.load(best_model_path))\n",
        "network.to(device)\n",
        "network.eval()\n",
        "print(\"Best model weights loaded successfully.\")\n",
        "\n",
        "# 2. Run Inference on a Sample Validation Image\n",
        "# We will use an image from the validation set (e.g., area10)\n",
        "sample_image_name = 'top_mosaic_09cm_area10.tif'\n",
        "\n",
        "# The produce_results function (defined in Cell 8) handles loading, prediction, and plotting.\n",
        "print(f\"\\nGenerating results for {sample_image_name}...\")\n",
        "produce_results(sample_image_name)"
      ],
      "metadata": {
        "id": "bYORklOxlaQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Ensure the best model is loaded and in evaluation mode\n",
        "network.load_state_dict(torch.load(best_model_path))\n",
        "network.to(device)\n",
        "network.eval()\n",
        "print(\"Starting full validation set evaluation...\")\n",
        "\n",
        "# Initialize a confusion matrix for all validation data\n",
        "total_confusion_matrix = np.zeros((len(class_labels), len(class_labels)), dtype=np.int64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        # Get predicted class\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Accumulate confusion matrix\n",
        "        total_confusion_matrix += compute_confusion_matrix(preds, labels, len(class_labels))\n",
        "\n",
        "# Calculate final metrics from the aggregated confusion matrix\n",
        "final_metrics = compute_metrics(total_confusion_matrix)\n",
        "\n",
        "print(\"\\n--- Final Metrics on Validation Set ---\")\n",
        "# The pretty_print_metrics function (defined in Cell 8) displays the final table\n",
        "pretty_print_metrics(final_metrics)\n",
        "\n",
        "print(\"\\nEvaluation complete.\")"
      ],
      "metadata": {
        "id": "xw914TgrlaNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Class labels defined in Cell 9: [0, 1, 2, 3, 4, 5]\n",
        "num_classes = len(class_labels)\n",
        "\n",
        "# Initialize counts for each class\n",
        "class_counts = np.zeros(num_classes, dtype=np.int64)\n",
        "\n",
        "print(\"Calculating class frequencies from training data...\")\n",
        "\n",
        "# Iterate through the entire training dataset to count pixels for each class\n",
        "for _, labels in tqdm(train_data, desc=\"Counting Pixels\"):\n",
        "    # Convert labels from tensor to numpy array\n",
        "    labels_np = labels.numpy()\n",
        "\n",
        "    # Count pixels for each class (0 to 5)\n",
        "    for i in range(num_classes):\n",
        "        class_counts[i] += np.sum(labels_np == i)\n",
        "\n",
        "print(\"\\nPixel counts per class:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Calculate weights: Inverse frequency scaling (median frequency balancing is also common)\n",
        "# Weight = Total Pixels / Class Count\n",
        "total_pixels = class_counts.sum()\n",
        "raw_weights = total_pixels / (class_counts + 1e-12) # Add small epsilon to prevent division by zero\n",
        "\n",
        "# Normalize weights so they sum up to the number of classes (optional, but good practice)\n",
        "weights = raw_weights / raw_weights.sum() * num_classes\n",
        "\n",
        "# Convert weights to a PyTorch tensor and move to the device\n",
        "class_weights = torch.from_numpy(weights).float().to(device)\n",
        "\n",
        "print(\"\\nCalculated Class Weights (to be used in loss function):\")\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "dMICxsNelaKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Redefine the criterion using the calculated weights\n",
        "criterion_weighted = nn.CrossEntropyLoss(weight=class_weights)\n",
        "print(\"Weighted CrossEntropyLoss initialized.\")\n",
        "\n",
        "# Redefine the optimizer, resetting learning rate and state\n",
        "optimizer_weighted = optim.Adam(network.parameters(), lr=0.001)\n",
        "print(\"Optimizer reset.\")"
      ],
      "metadata": {
        "id": "9EZtH1vQlaIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# NOTE: Requires base_folder, criterion_weighted, optimizer_weighted, criterion (unweighted),\n",
        "# train_loader, val_loader, and class_labels to be defined in previous cells.\n",
        "\n",
        "# --- Setup for Model Saving (New Path for Weighted Model) ---\n",
        "# NOTE: Ensure 'base_folder' is defined for persistent saving\n",
        "save_directory = os.path.join(base_folder, 'WeightsVaihingen')\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "best_model_path_weighted = os.path.join(save_directory, 'best_hypercolumns_weighted_model.pth')\n",
        "\n",
        "# --- Metric Initialization ---\n",
        "train_losses_w = []\n",
        "val_losses_w = []\n",
        "val_ious_w = []\n",
        "best_iou_w = -1.0\n",
        "\n",
        "print(f\"Starting Weighted Training for {number_epochs} epochs...\")\n",
        "for epoch in range(number_epochs): # This loop now runs 25 times\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{number_epochs} (Weighted) ---\")\n",
        "\n",
        "    # Training step - using the weighted criterion and optimizer\n",
        "    train_loss = train_model(network, train_loader, criterion_weighted, optimizer_weighted, device)\n",
        "    train_losses_w.append(train_loss)\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validation step - always use the unweighted criterion for fair loss comparison\n",
        "    val_loss, val_iou = val_model(network, val_loader, criterion, device, class_labels)\n",
        "    val_losses_w.append(val_loss)\n",
        "    val_ious_w.append(val_iou)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Mean IoU: {val_iou:.4f}\")\n",
        "\n",
        "    # Save the best model based on Mean IoU\n",
        "    if val_iou > best_iou_w:\n",
        "        best_iou_w = val_iou\n",
        "        # Save model state dict to the designated Drive location\n",
        "        torch.save(network.state_dict(), best_model_path_weighted)\n",
        "        print(f\"Model saved to {best_model_path_weighted} with new best IoU: {best_iou_w:.4f}\")\n",
        "\n",
        "print(\"\\nWeighted Training complete.\")"
      ],
      "metadata": {
        "id": "sUT0apfblaFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# The path to the best model from the weighted training run (defined in the last step)\n",
        "# NOTE: Ensure best_model_path_weighted is defined and points to the correct weights!\n",
        "\n",
        "# 1. Load the Best Weighted Model Weights\n",
        "print(f\"Loading weighted model weights from: {best_model_path_weighted}\")\n",
        "network.load_state_dict(torch.load(best_model_path_weighted))\n",
        "network.to(device)\n",
        "network.eval()\n",
        "\n",
        "# 2. Run Full Evaluation\n",
        "print(\"\\nStarting full validation set evaluation of the WEIGHTED MODEL...\")\n",
        "\n",
        "# Initialize a fresh confusion matrix\n",
        "total_confusion_matrix_weighted = np.zeros((len(class_labels), len(class_labels)), dtype=np.int64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating Weighted\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = network(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Accumulate confusion matrix\n",
        "        total_confusion_matrix_weighted += compute_confusion_matrix(preds, labels, len(class_labels))\n",
        "\n",
        "# 3. Calculate and Print Final Metrics\n",
        "final_metrics_weighted = compute_metrics(total_confusion_matrix_weighted)\n",
        "\n",
        "print(\"\\n--- Final Metrics on Validation Set (WEIGHTED LOSS) ---\")\n",
        "# Use the pretty print function from the original notebook's Cell 31 (now Cell 31/32 equivalent)\n",
        "pretty_print_metrics(final_metrics_weighted)\n",
        "\n",
        "print(\"\\nEvaluation of weighted model complete.\")"
      ],
      "metadata": {
        "id": "6sKCbZbelaCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# We will use the best weighted model\n",
        "network.load_state_dict(torch.load(best_model_path_weighted))\n",
        "network.to(device)\n",
        "network.eval()\n",
        "\n",
        "def tiled_prediction_and_display(network, image_name, patch_size=256, overlap=32, device=device):\n",
        "    \"\"\"\n",
        "    Performs inference using an overlapping tile strategy and displays results.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Setup ---\n",
        "    image_path = os.path.join(DATA_ROOT_DIR, 'top', image_name)\n",
        "    GT_path    = os.path.join(DATA_ROOT_DIR, '1CGT', image_name)\n",
        "\n",
        "    # Load data\n",
        "    image = io.imread(image_path)\n",
        "    image_rgb = image[:, :, :3]\n",
        "    GT = io.imread(GT_path).astype(int)\n",
        "\n",
        "    H, W, C = image_rgb.shape\n",
        "\n",
        "    # Initialize the output prediction map\n",
        "    predicted_mask = np.zeros((H, W), dtype=np.uint8)\n",
        "\n",
        "    # Define stride for tiling\n",
        "    stride = patch_size - overlap\n",
        "\n",
        "    # Calculate starting points for rows and columns\n",
        "    r_starts = np.arange(0, H, stride)\n",
        "    c_starts = np.arange(0, W, stride)\n",
        "\n",
        "    # Adjust starts to ensure the last patch fully covers the image edge\n",
        "    if r_starts[-1] + patch_size < H:\n",
        "        r_starts = np.append(r_starts, H - patch_size)\n",
        "    else:\n",
        "        r_starts[-1] = H - patch_size\n",
        "\n",
        "    if c_starts[-1] + patch_size < W:\n",
        "        c_starts = np.append(c_starts, W - patch_size)\n",
        "    else:\n",
        "        c_starts[-1] = W - patch_size\n",
        "\n",
        "    r_starts = np.unique(r_starts)\n",
        "    c_starts = np.unique(c_starts)\n",
        "\n",
        "    total_tiles = len(r_starts) * len(c_starts)\n",
        "\n",
        "    print(f\"Total tiles to process: {total_tiles} ({len(r_starts)} rows x {len(c_starts)} cols)\")\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for r_start in tqdm(r_starts, desc=\"Tiling Rows\"):\n",
        "            for c_start in c_starts:\n",
        "\n",
        "                # Extract patch\n",
        "                patch_img = image_rgb[r_start:r_start + patch_size, c_start:c_start + patch_size, :3]\n",
        "\n",
        "                # Preprocessing\n",
        "                img_tensor = patch_img.astype(np.float32) / 255.0\n",
        "                img_tensor = np.transpose(img_tensor, (2, 0, 1))\n",
        "                img_tensor = torch.from_numpy(img_tensor).unsqueeze(0).to(device)\n",
        "\n",
        "                # Inference\n",
        "                output = network(img_tensor)\n",
        "\n",
        "                # Post-processing\n",
        "                output = output.squeeze().cpu()\n",
        "                _, pred_patch = torch.max(output, 0)\n",
        "                pred_patch = pred_patch.numpy().astype(np.uint8)\n",
        "\n",
        "                # --- 3. Stitching (Simple Overwrite) ---\n",
        "                r_end = r_start + patch_size\n",
        "                c_end = c_start + patch_size\n",
        "\n",
        "                predicted_mask[r_start:r_end, c_start:c_end] = pred_patch\n",
        "\n",
        "    # --- 4. Plotting ---\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.title(f'Original Image: {image_name}')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(GT)\n",
        "    plt.title('Ground Truth')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(predicted_mask)\n",
        "    plt.title('Tiled Predicted Mask (Weighted Model)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run the Tiled Inference on a sample image (e.g., area10 from validation set )\n",
        "sample_image_name = 'top_mosaic_09cm_area10.tif'\n",
        "tiled_prediction_and_display(network, sample_image_name, patch_size=256, overlap=32, device=device)"
      ],
      "metadata": {
        "id": "T6YvYEKwlZ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UXrKX_oRmd5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xpEhxEmhmdqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4q4qTktsmdQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "U NET"
      ],
      "metadata": {
        "id": "z1jo5jRamcXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Basic Convolutional Block\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(Convolution => BN => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "# 2. Downsampling Block\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "# 3. Corrected Upsampling Block (FIXED: Handles channels for x1 and x2 separately)\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv with correct channel calculation\"\"\"\n",
        "    def __init__(self, in_channels_x1, in_channels_x2, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. The upsampling layer operates only on x1 channels\n",
        "        self.up = nn.ConvTranspose2d(in_channels_x1, in_channels_x1, kernel_size=2, stride=2)\n",
        "\n",
        "        # 2. The subsequent double convolution takes the concatenated tensor\n",
        "        # Total channels = x2 (skip connection) + x1_upsampled\n",
        "        self.conv = DoubleConv(in_channels_x2 + in_channels_x1, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Upsample x1\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Concatenate x2 (skip connection) and x1 (upsampled features)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "\n",
        "        return self.conv(x)\n",
        "\n",
        "# 4. Final U-Net Model (FIXED: Instantiates Up blocks with correct channel arguments)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=6):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Encoder (Contracting Path)\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 512)\n",
        "\n",
        "        # Decoder (Expanding Path) - MODIFIED CHANNEL CALLS\n",
        "        # Up(in_channels_x1, in_channels_x2, out_channels)\n",
        "        self.up1 = Up(512, 512, 256)\n",
        "        self.up2 = Up(256, 256, 128)\n",
        "        self.up3 = Up(128, 128, 64)\n",
        "        self.up4 = Up(64, 64, 64)\n",
        "\n",
        "        # Output Layer\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        # Decoder + Skip Connections\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "\n",
        "        # Final Classification\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "fWemTSL5mlRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "# NOTE: VaihingenDataset and helper functions (Code 7 & 8) must be defined and run prior to this.\n",
        "\n",
        "# =========================================================\n",
        "# 1. Configuration and Paths\n",
        "# =========================================================\n",
        "\n",
        "# Ensure your paths are correct in your Google Drive.\n",
        "DATA_ROOT_DIR = \"/content/drive/MyDrive/Workshop_Semantic_Segmentation/DataVaihingen\"\n",
        "base_folder = \"/content/drive/MyDrive/Workshop_Semantic_Segmentation\"\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define Hyperparameters\n",
        "class_labels = [0, 1, 2, 3, 4, 5]\n",
        "patch_size = 256\n",
        "batch_size = 4\n",
        "number_epochs = 25\n",
        "\n",
        "# Define the folder paths within the root directory\n",
        "IMG_FOLDER = os.path.join(DATA_ROOT_DIR, 'top')\n",
        "GT_FOLDER = os.path.join(DATA_ROOT_DIR, '1CGT')\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2. Initialize Data Loaders\n",
        "# =========================================================\n",
        "\n",
        "# Create Datasets (VaihingenDataset from Code 7)\n",
        "train_data = VaihingenDataset(IMG_FOLDER, GT_FOLDER, split='train', patch_size=patch_size)\n",
        "val_data = VaihingenDataset(IMG_FOLDER, GT_FOLDER, split='val', patch_size=patch_size)\n",
        "\n",
        "# Create DataLoaders (with num_workers=0 fix)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. Initialize Model, Loss, and Optimizer\n",
        "# =========================================================\n",
        "\n",
        "# Initialize Model: UNet is instantiated here\n",
        "network = UNet(n_channels=3, n_classes=len(class_labels))\n",
        "network.to(device)\n",
        "print(\"U-Net Network initialized and moved to device.\")\n",
        "\n",
        "# Initialize Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Criterion and Optimizer initialized.\")"
      ],
      "metadata": {
        "id": "U6FYMrJ-mk9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# =========================================================================\n",
        "# SETUP\n",
        "# =========================================================================\n",
        "\n",
        "# Variables (Assumed from Code 9)\n",
        "number_epochs = 25\n",
        "base_folder = \"/content/drive/MyDrive/Workshop_Semantic_Segmentation\"\n",
        "\n",
        "# Setup for Model Saving - SAVES TO GOOGLE DRIVE\n",
        "save_directory = os.path.join(base_folder, 'WeightsVaihingen_UNet')\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "best_model_path = os.path.join(save_directory, 'best_UNet_model.pth')\n",
        "\n",
        "# Metric Initialization\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_ious = []\n",
        "best_iou = -1.0\n",
        "\n",
        "# =========================================================================\n",
        "# BASELINE TRAINING LOOP\n",
        "# =========================================================================\n",
        "\n",
        "print(f\"Starting U-Net training for {number_epochs} epochs...\")\n",
        "for epoch in range(number_epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{number_epochs} ---\")\n",
        "\n",
        "    # Training step\n",
        "    train_loss = train_model(network, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validation step\n",
        "    val_loss, val_iou = val_model(network, val_loader, criterion, device, class_labels)\n",
        "    val_losses.append(val_loss)\n",
        "    val_ious.append(val_iou)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Mean IoU: {val_iou:.4f}\")\n",
        "\n",
        "    # Save the best model based on Mean IoU\n",
        "    if val_iou > best_iou:\n",
        "        best_iou = val_iou\n",
        "        # Save model state dict to the designated Drive location\n",
        "        torch.save(network.state_dict(), best_model_path)\n",
        "        print(f\"Model saved to {best_model_path} with new best IoU: {best_iou:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")"
      ],
      "metadata": {
        "id": "m5mF6CvelZ8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# =========================================================================\n",
        "# 1. Load the Best Model Weights\n",
        "# =========================================================================\n",
        "\n",
        "# Ensure the best_model_path points to the file saved by the training loop\n",
        "best_model_path = os.path.join(base_folder, 'WeightsVaihingen_UNet', 'best_UNet_model.pth')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Re-instantiate the UNet model (which was defined in Code 1)\n",
        "try:\n",
        "    # NOTE: The UNet class must be defined in your notebook scope\n",
        "    network.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    network.eval()\n",
        "    print(f\"Successfully loaded best UNet model from: {best_model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {best_model_path}. Did training finish?\")\n",
        "except NameError:\n",
        "    print(\"Error: The 'UNet' class definition must be executed before loading the model.\")\n",
        "\n",
        "# =========================================================================\n",
        "# 2. Plot Training History\n",
        "# =========================================================================\n",
        "\n",
        "def plot_metrics(train_losses, val_losses, val_ious):\n",
        "    \"\"\"Plots the loss and IoU curves over epochs.\"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'r-o', label='Training Loss')\n",
        "    # Check if val_losses is populated before plotting\n",
        "    if val_losses and any(v is not None for v in val_losses):\n",
        "        plt.plot(epochs, val_losses, 'b-o', label='Validation Loss')\n",
        "\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot IoU\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Ensure val_ious is a list of numbers\n",
        "    if val_ious and all(isinstance(i, (int, float)) for i in val_ious):\n",
        "        plt.plot(epochs, val_ious, 'g-o', label='Mean IoU')\n",
        "        plt.title('Mean Intersection over Union (IoU)')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('IoU')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    else:\n",
        "        plt.title('Mean IoU (Data Missing or Invalid)')\n",
        "        print(\"\\nWarning: IoU metrics were not plotted correctly. Check your val_model return values.\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nPlotting training history...\")\n",
        "plot_metrics(train_losses, val_losses, val_ious)\n",
        "\n",
        "# =========================================================================\n",
        "# 3. Visualization Placeholder (Requires your test image data)\n",
        "# =========================================================================\n",
        "print(\"\\n--- Model Comparison Summary ---\")\n",
        "print(f\"Best U-Net Mean IoU achieved: {best_iou:.4f}\")\n",
        "# You should manually compare this to the best_iou from your Hypercolumns run!\n",
        "\n",
        "# NOTE: This section requires you to load a test image and its ground truth\n",
        "# to display a side-by-side comparison.\n",
        "\n",
        "\"\"\"\n",
        "# --- Visualization Placeholder ---\n",
        "\n",
        "# 1. Load a test image (e.g., test_input) and its ground truth (e.g., test_gt)\n",
        "#    from your test set. (This data loading code needs to be added by you.)\n",
        "# test_input, test_gt = load_single_test_image()\n",
        "\n",
        "# 2. Preprocess and run inference\n",
        "# with torch.no_grad():\n",
        "#     test_input = test_input.to(device).unsqueeze(0) # Add batch dimension\n",
        "#     output = network(test_input)\n",
        "#     _, prediction = torch.max(output, 1)\n",
        "#     prediction = prediction.cpu().squeeze(0).numpy()\n",
        "\n",
        "# 3. Display results\n",
        "# visualize_prediction(test_input.cpu().squeeze(0), test_gt, prediction)\n",
        "\"\"\"\n",
        "print(\"Proceed to add your final visualization code to compare input, GT, and prediction.\")"
      ],
      "metadata": {
        "id": "ysp78XvYlZ6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# =========================================================================\n",
        "# 1. Load the Best Model Weights\n",
        "# =========================================================================\n",
        "\n",
        "# Ensure the best_model_path points to the file saved by the training loop\n",
        "best_model_path = os.path.join(base_folder, 'WeightsVaihingen_UNet', 'best_UNet_model.pth')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Re-instantiate the UNet model (which was defined in Code 1)\n",
        "try:\n",
        "    # NOTE: The UNet class must be defined in your notebook scope\n",
        "    network.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    network.eval()\n",
        "    print(f\"Successfully loaded best UNet model from: {best_model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {best_model_path}. Did training finish?\")\n",
        "except NameError:\n",
        "    print(\"Error: The 'UNet' class definition must be executed before loading the model.\")\n",
        "\n",
        "# =========================================================================\n",
        "# 2. Plot Training History\n",
        "# =========================================================================\n",
        "\n",
        "def plot_metrics(train_losses, val_losses, val_ious):\n",
        "    \"\"\"Plots the loss and IoU curves over epochs.\"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'r-o', label='Training Loss')\n",
        "    # Check if val_losses is populated before plotting\n",
        "    if val_losses and any(v is not None for v in val_losses):\n",
        "        plt.plot(epochs, val_losses, 'b-o', label='Validation Loss')\n",
        "\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot IoU\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Ensure val_ious is a list of numbers\n",
        "    if val_ious and all(isinstance(i, (int, float)) for i in val_ious):\n",
        "        plt.plot(epochs, val_ious, 'g-o', label='Mean IoU')\n",
        "        plt.title('Mean Intersection over Union (IoU)')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('IoU')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    else:\n",
        "        plt.title('Mean IoU (Data Missing or Invalid)')\n",
        "        print(\"\\nWarning: IoU metrics were not plotted correctly. Check your val_model return values.\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nPlotting training history...\")\n",
        "plot_metrics(train_losses, val_losses, val_ious)\n",
        "\n",
        "# =========================================================================\n",
        "# 3. Model Comparison Summary\n",
        "# =========================================================================\n",
        "print(\"\\n--- Model Comparison Summary ---\")\n",
        "print(f\"Best U-Net Mean IoU achieved: {best_iou:.4f}\")\n",
        "print(\"Compare this Mean IoU to your Hypercolumns Mean IoU to determine which model performed better!\")\n",
        "print(\"Next, add your custom code for visual inspection of a test image.\")"
      ],
      "metadata": {
        "id": "ufgCnqKUlZ3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# HELPER FUNCTION FOR VISUALIZATION (Color Mapping - MUST be run here)\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def visualize_prediction(image, gt, prediction, iou):\n",
        "    \"\"\"\n",
        "    Plots the input image, ground truth, and prediction mask.\n",
        "    \"\"\"\n",
        "    # RGB color mapping for the 6 classes\n",
        "    COLOR_MAP = np.array([\n",
        "        [255, 255, 255], # 0: Impervious Surfaces (White)\n",
        "        [0, 0, 255],     # 1: Building (Blue)\n",
        "        [0, 255, 255],   # 2: Low Vegetation (Cyan)\n",
        "        [0, 255, 0],     # 3: Tree (Green)\n",
        "        [255, 255, 0],   # 4: Car (Yellow)\n",
        "        [255, 0, 0]      # 5: Clutter/Background (Red)\n",
        "    ], dtype=np.uint8)\n",
        "\n",
        "    def map_index_to_color(mask):\n",
        "        color_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
        "        for i in range(len(COLOR_MAP)):\n",
        "            color_mask[mask == i] = COLOR_MAP[i]\n",
        "        return color_mask\n",
        "\n",
        "    # 1. Prepare data for plotting\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    gt_colored = map_index_to_color(gt)\n",
        "    pred_colored = map_index_to_color(prediction)\n",
        "\n",
        "    # 2. Plotting\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    axes[0].imshow(image_np)\n",
        "    axes[0].set_title('Input Image (RGB)')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(gt_colored)\n",
        "    axes[1].set_title('Ground Truth')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(pred_colored)\n",
        "    axes[2].set_title(f'U-Net Prediction (Mean IoU: {iou:.4f})')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# INFERENCE EXECUTION\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nRunning visual inspection on a validation patch for U-Net...\")\n",
        "\n",
        "# 1. Grab one patch from the validation dataset\n",
        "try:\n",
        "    val_iterator = iter(val_loader)\n",
        "    inputs, labels = next(val_iterator)\n",
        "except NameError:\n",
        "    print(\"Error: val_loader is not defined. Ensure Code 9 was run.\")\n",
        "    raise\n",
        "\n",
        "# Select the first image in the batch for visualization\n",
        "input_patch = inputs[0]\n",
        "gt_mask = labels[0].cpu().numpy()\n",
        "\n",
        "# 2. Preprocess and Run Inference\n",
        "network.to(device)\n",
        "network.eval()\n",
        "with torch.no_grad():\n",
        "    input_tensor = input_patch.to(device).unsqueeze(0)\n",
        "    output = network(input_tensor)\n",
        "    _, prediction_mask = torch.max(output, 1)\n",
        "    prediction_mask = prediction_mask.cpu().squeeze(0).numpy()\n",
        "\n",
        "# 3. Visualize\n",
        "# Assuming best_iou holds the U-Net's best IoU value from Code 11\n",
        "visualize_prediction(input_patch, gt_mask, prediction_mask, best_iou)\n",
        "\n",
        "print(\"\\nU-Net visual comparison generated.\")"
      ],
      "metadata": {
        "id": "i318sWyhlZ0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# NOTE: The Hypercolumns class MUST be defined in your notebook scope\n",
        "# (from the initial phase of the project).\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1. Load the Best Hypercolumns Model Weights\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Path for the Hypercolumns weights (Assuming the default folder name from the first run)\n",
        "hypercolumns_best_path = os.path.join(base_folder, 'WeightsVaihingen', 'best_model.pth')\n",
        "print(f\"Attempting to load Hypercolumns model from: {hypercolumns_best_path}\")\n",
        "\n",
        "# Re-instantiate the Hypercolumns model\n",
        "try:\n",
        "    # IMPORTANT: You must replace 'Hypercolumns' with the actual class name if it was different.\n",
        "    hypercolumns_network = Hypercolumns(num_classes=len(class_labels))\n",
        "    hypercolumns_network.load_state_dict(torch.load(hypercolumns_best_path, map_location=device))\n",
        "    hypercolumns_network.to(device)\n",
        "    hypercolumns_network.eval()\n",
        "    print(\"Successfully loaded and set Hypercolumns model to evaluation mode.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nError: The 'Hypercolumns' class is not defined. Please define it and run again.\")\n",
        "    raise\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: Hypercolumns model file not found at {hypercolumns_best_path}. Check save path.\")\n",
        "    raise\n",
        "\n",
        "# Placeholder for Hypercolumns IoU (REPLACE WITH YOUR ACTUAL VALUE)\n",
        "# You MUST manually find and replace '0.5000' with the best IoU score achieved by your Hypercolumns model!\n",
        "hypercolumns_best_iou = 0.5000\n",
        "print(f\"Hypercolumns Best IoU placeholder: {hypercolumns_best_iou:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# INFERENCE EXECUTION\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nRunning visual inspection for the Hypercolumns model on the same validation patch...\")\n",
        "\n",
        "# 1. Run Inference (using the same input_patch tensor loaded in the previous block)\n",
        "with torch.no_grad():\n",
        "    input_tensor = input_patch.to(device).unsqueeze(0) # Input is the same\n",
        "    output = hypercolumns_network(input_tensor)\n",
        "    _, prediction_mask = torch.max(output, 1)\n",
        "    prediction_mask = prediction_mask.cpu().squeeze(0).numpy()\n",
        "\n",
        "# 2. Visualize (using the same visualization function from the previous block)\n",
        "# The function is defined in your previous cell and is now ready to use.\n",
        "visualize_prediction(input_patch, gt_mask, prediction_mask, hypercolumns_best_iou)\n",
        "\n",
        "print(\"\\nVisual comparison for Hypercolumns complete.\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3. Final Summary\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\n--- Project Conclusion ---\")\n",
        "print(\"You have successfully trained, analyzed, and visualized both the Hypercolumns and U-Net models.\")\n",
        "print(\"Final comparison:\")\n",
        "print(f\"1. U-Net Best Mean IoU: {best_iou:.4f}\")\n",
        "print(f\"2. Hypercolumns Best Mean IoU: {hypercolumns_best_iou:.4f}\")\n",
        "print(\"\\nCompare the two prediction images visually and compare the IoU scores to draw your conclusion.\")"
      ],
      "metadata": {
        "id": "Ftdb2yxElZx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDOU7doklZvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHSlbrSNlZsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbXUPCONlZpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkdveuEDlZm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYF5AKi3lZkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Yq7lMLGlZhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXwBE5YBlZfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hGvx40UHlZcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDXcOZ2TlZZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_UmjmThSlZXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bI1ujeu8lZVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQ2WWXN0lZRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZwQvepFlZPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZfBIdmrlZMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7FLiTiolZJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LI8LM7XdlZG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2t2SiVTXlZE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s4KlRD3clZB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgtgbUk6lY_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vh9-IIMwlY81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3K9pgHGlY6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DnrF4MR-lY3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QOaA9aIWlYyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOaZ0YmtlYvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j6xL6fWqlYr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}